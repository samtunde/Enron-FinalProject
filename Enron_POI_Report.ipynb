{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background of Enron Fraud\n",
    "The Enron scandal, publicized in October 2001, eventually led to the bankruptcy of the Enron Corporation, an American energy company based in Houston, Texas, and the de facto dissolution of Arthur Andersen, which was one of the five largest audit and accountancy partnerships in the world. In addition to being the largest bankruptcy reorganization in American history at that time, Enron was cited as the biggest audit failure.[1]\n",
    "\n",
    "Enron was formed in 1985 by Kenneth Lay after merging Houston Natural Gas and InterNorth. Several years later, when Jeffrey Skilling was hired, he developed a staff of executives that – by the use of accounting loopholes, special purpose entities, and poor financial reporting – were able to hide billions of dollars in debt from failed deals and projects. Chief Financial Officer Andrew Fastow and other executives not only misled Enron's Board of Directors and Audit Committee on high-risk accounting practices, but also pressured Arthur Andersen to ignore the issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samtunde/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat\n",
    "from feature_format import targetFeatureSplit\n",
    "import tester\n",
    "from tester import dump_classifier_and_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Dataset and Question\n",
    "    The goal in this section is to go through the learning, cleaning and preparing of the data. Next, selecting features which influence mostly on the target, creating new features which explain target the better. This section also involves outlier investigations and data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 19 columns):\n",
      "poi                          146 non-null bool\n",
      "salary                       95 non-null float64\n",
      "bonus                        82 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "deferral_payments            39 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "other                        93 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "exercised_stock_options      102 non-null float64\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "dtypes: bool(1), float64(18)\n",
      "memory usage: 21.8+ KB\n"
     ]
    }
   ],
   "source": [
    "features_list = ['poi',\n",
    "                'salary',\n",
    "                'bonus', \n",
    "                'long_term_incentive', \n",
    "                'deferred_income', \n",
    "                'deferral_payments',\n",
    "                'loan_advances', \n",
    "                'other',\n",
    "                'expenses', \n",
    "                'director_fees',\n",
    "                'total_payments',\n",
    "                'exercised_stock_options',\n",
    "                'restricted_stock',\n",
    "                'restricted_stock_deferred',\n",
    "                'total_stock_value',\n",
    "                'to_messages',\n",
    "                'from_messages',\n",
    "                'from_this_person_to_poi',\n",
    "                'from_poi_to_this_person']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# Transform data from dictionary to the Pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(data_dict, orient = 'index')\n",
    "#Order columns in DataFrame, exclude email column\n",
    "df = df[features_list]\n",
    "df = df.replace('NaN', np.nan)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_advances                  4\n",
       "director_fees                 17\n",
       "restricted_stock_deferred     18\n",
       "deferral_payments             39\n",
       "deferred_income               49\n",
       "long_term_incentive           66\n",
       "bonus                         82\n",
       "from_messages                 86\n",
       "to_messages                   86\n",
       "from_poi_to_this_person       86\n",
       "from_this_person_to_poi       86\n",
       "other                         93\n",
       "expenses                      95\n",
       "salary                        95\n",
       "exercised_stock_options      102\n",
       "restricted_stock             110\n",
       "total_payments               125\n",
       "total_stock_value            126\n",
       "poi                          146\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of NaN values in the dataset:  1263\n"
     ]
    }
   ],
   "source": [
    "print \"Total number of NaN values in the dataset: \", df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace NaN in financial features with 0\n",
    "df.ix[:,:15] = df.ix[:,:15].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of NaN values in the dataset:  240\n"
     ]
    }
   ],
   "source": [
    "print \"Total number of NaN values in the dataset: \", df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Data Accuracy\n",
    "    Question: Does the summation of the payment features equal total payments? If not list the employees where this is the case. I'm going to check the accuracy of the financial data by summing up the payment features and comparing it with the total_payment feature and stock features and comparing with the total_stock_value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [poi, salary, bonus, long_term_incentive, deferred_income, deferral_payments, loan_advances, other, expenses, director_fees, total_payments, exercised_stock_options, restricted_stock, restricted_stock_deferred, total_stock_value, to_messages, from_messages, from_this_person_to_poi, from_poi_to_this_person]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split of POI and non-POI in the dataset\n",
    "poi_non_poi = df.poi.value_counts()\n",
    "poi_non_poi.index=['non-POI', 'POI']\n",
    "\n",
    "# Replace NaN in financial features with 0\n",
    "df.ix[:,:15] = df.ix[:,:15].fillna(0)\n",
    "\n",
    "email_features = ['to_messages', 'from_messages', 'from_this_person_to_poi', 'from_poi_to_this_person']\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "\n",
    "#impute missing values of email features \n",
    "df.loc[df[df.poi == 1].index,email_features] = imp.fit_transform(df[email_features][df.poi == 1])\n",
    "df.loc[df[df.poi == 0].index,email_features] = imp.fit_transform(df[email_features][df.poi == 0])\n",
    "\n",
    "## Review financial data accuracy\n",
    "#check data: summing payments features and compare with total_payments\n",
    "payments = ['salary',\n",
    "            'bonus', \n",
    "            'long_term_incentive', \n",
    "            'deferred_income', \n",
    "            'deferral_payments',\n",
    "            'loan_advances', \n",
    "            'other',\n",
    "            'expenses', \n",
    "            'director_fees']\n",
    "df[df[payments].sum(axis='columns') != df.total_payments]\n",
    "\n",
    "stock_value = ['exercised_stock_options',\n",
    "                'restricted_stock',\n",
    "                'restricted_stock_deferred']\n",
    "df[df[stock_value].sum(axis='columns') != df.total_stock_value]\n",
    "\n",
    "## Update financial data for BHATNAGAR SANJAY and BELFER ROBERT\n",
    "df.ix['BELFER ROBERT','total_payments'] = 3285\n",
    "df.ix['BELFER ROBERT','deferral_payments'] = 0\n",
    "df.ix['BELFER ROBERT','restricted_stock'] = 44093\n",
    "df.ix['BELFER ROBERT','restricted_stock_deferred'] = -44093\n",
    "df.ix['BELFER ROBERT','total_stock_value'] = 0\n",
    "df.ix['BELFER ROBERT','director_fees'] = 102500\n",
    "df.ix['BELFER ROBERT','deferred_income'] = -102500\n",
    "df.ix['BELFER ROBERT','exercised_stock_options'] = 0\n",
    "df.ix['BELFER ROBERT','expenses'] = 3285\n",
    "df.ix['BELFER ROBERT',]\n",
    "df.ix['BHATNAGAR SANJAY','expenses'] = 137864\n",
    "df.ix['BHATNAGAR SANJAY','total_payments'] = 137864\n",
    "df.ix['BHATNAGAR SANJAY','exercised_stock_options'] = 1.54563e+07\n",
    "df.ix['BHATNAGAR SANJAY','restricted_stock'] = 2.60449e+06\n",
    "df.ix['BHATNAGAR SANJAY','restricted_stock_deferred'] = -2.60449e+06\n",
    "df.ix['BHATNAGAR SANJAY','other'] = 0\n",
    "df.ix['BHATNAGAR SANJAY','director_fees'] = 0\n",
    "df.ix['BHATNAGAR SANJAY','total_stock_value'] = 1.54563e+07\n",
    "df.ix['BHATNAGAR SANJAY']\n",
    "df[df[payments].sum(axis='columns') != df.total_payments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of NaN values in the dataset:  0\n"
     ]
    }
   ],
   "source": [
    "#df = df[features_list]\n",
    "#df.head(10)\n",
    "print \"Total number of NaN values in the dataset: \", df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Investigation\n",
    "Now the data has been cleaned from missing values and typos I would like to discover the outliers. Descriptive statistics can be used to determine outliers of the distibution as the values which are higher than Q2 + 1.5IQR or less than Q2 - 1.5IQR, where Q2 median of the distribution, IQR - interquartile range. I'm going to calculate the sum of outlier variables for each person and sort them descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEKCAYAAABQRFHsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119d80210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize outliers\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "features = [\"salary\", \"bonus\"]\n",
    "\n",
    "## remove the outlier key = TOTAL\n",
    "data_dict.pop('TOTAL', 0)\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    plt.scatter(salary, bonus, c=None)\n",
    "\n",
    "plt.xlabel(\"Salary\")\n",
    "plt.ylabel(\"Bonus\")\n",
    "plt.show()\n",
    "\n",
    "## Using descriptive statistics to determine the outliers in this distribution of data\n",
    "outliers = df.quantile(.5) + 1.5 * (df.quantile(.75)-df.quantile(.25))\n",
    "pd.DataFrame((df[1:] > outliers[1:]).sum(axis = 1), columns = ['# of outliers']).\\\n",
    "    sort_values('# of outliers',  ascending = [0]).head(12)\n",
    "    \n",
    "    \n",
    "# Remove TOTAL outlier from the data set\n",
    "df = df.drop(['TOTAL'],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Review:\n",
    "The first value is 'TOTAL' which is the total value of financial payments from the FindLaw data. As it's doesn't make any sence for our solution, I'm going to exclude it from the data set.\n",
    "\n",
    "Kenneth Lay and Jeffrey Skilling are very well known persons from ENRON scandal. They will be kept in the dataset as they represent anomalies but not the outliers.\n",
    "\n",
    "Mark Frevert and Lawrence Whalley are high level managers at Enron who could represent great examples for modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new feature: fraction of person's email to POI to all sent messages\n",
    "df['fraction_to_poi'] = df['from_this_person_to_poi']/df['from_messages']\n",
    "\n",
    "# Clean all 'inf' values which we got if the person's from_messages = 0\n",
    "df = df.replace('inf', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create New Feature\n",
    "I reviewed multiple resource allocation (POI/NON_POI) of the financial data and very little insight to the data. An email feature was created to check the fraction of emails, sent to POI, to all sent emails; emails, received from POI, to all received emails. The new feature name is \"fraction_to_poi\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### create new features\n",
    "### new features are: fraction_to_poi_email,fraction_from_poi_email\n",
    "#from IPython.display import Image\n",
    "#import matplotlib.pyplot as plt\n",
    "def dict_to_list(key,normalizer):\n",
    "    new_list=[]\n",
    "\n",
    "    for i in data_dict:\n",
    "        if data_dict[i][key]==\"NaN\" or data_dict[i][normalizer]==\"NaN\":\n",
    "            new_list.append(0.)\n",
    "        elif data_dict[i][key]>=0:\n",
    "            new_list.append(float(data_dict[i][key])/float(data_dict[i][normalizer]))\n",
    "    return new_list\n",
    "\n",
    "### create two lists of new features\n",
    "fraction_from_poi_email=dict_to_list(\"from_poi_to_this_person\",\"to_messages\")\n",
    "fraction_to_poi_email=dict_to_list(\"from_this_person_to_poi\",\"from_messages\")\n",
    "\n",
    "### insert new features into data_dict\n",
    "count=0\n",
    "for i in data_dict:\n",
    "    data_dict[i][\"fraction_from_poi_email\"]=fraction_from_poi_email[count]\n",
    "    data_dict[i][\"fraction_to_poi_email\"]=fraction_to_poi_email[count]\n",
    "    count +=1\n",
    "\n",
    "    \n",
    "#features_list = [\"poi\", \"fraction_from_poi_email\", \"fraction_to_poi_email\"]    \n",
    "    ### store to my_dataset for easy export below\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "In order to find the most effective features for classification, feature selection using “Decision Tree” was deployed to rank the features. Resulting in a number of features with non-null feature importance, sorted by importance. \n",
    "\n",
    "Note Decision tree doesn't require me any feature scaling. According to feature_importances attribute of the classifier, just created fraction_to_poi feature has the highest importance for the model. It is important to note the number of features used for the model can cause varied results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.733333333333\n",
      "Decision tree algorithm time: 0.004 s\n",
      "Feature Ranking: \n",
      "1 salary (0.18913356647)\n",
      "2 bonus (0.173931999677)\n",
      "3 long_term_incentive (0.144261508951)\n",
      "4 deferred_income (0.142220496894)\n",
      "5 deferral_payments (0.141375325566)\n",
      "6 loan_advances (0.0846048429142)\n",
      "7 other (0.0710561104583)\n",
      "8 expenses (0.0534161490683)\n",
      "9 director_fees (0.0)\n",
      "10 total_payments (0.0)\n",
      "11 exercised_stock_options (0.0)\n",
      "12 restricted_stock (0.0)\n",
      "13 restricted_stock_deferred (0.0)\n",
      "14 total_stock_value (0.0)\n",
      "15 to_messages (0.0)\n",
      "16 from_messages (0.0)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "## features_list = [\"poi\", \"salary\", \"bonus\", \"fraction_from_poi_email\", \"fraction_to_poi_email\",\n",
    "#                 'deferral_payments', 'total_payments', 'loan_advances', 'restricted_stock_deferred',\n",
    "#                 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options',\n",
    " #                'long_term_incentive', 'shared_receipt_with_poi', 'restricted_stock', 'director_fees']\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "\n",
    "### split into labels and features (this line assumes that the first\n",
    "### feature in the array is the label, which is why \"poi\" must always\n",
    "### be first in features_list\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### split data into training and testing datasets\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, \n",
    "                                                                labels, test_size=0.1, random_state=42)\n",
    "t0 = time()\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train,labels_train)\n",
    "score = clf.score(features_test,labels_test)\n",
    "pred= clf.predict(features_test)\n",
    "print 'accuracy', score\n",
    "\n",
    "print \"Decision tree algorithm time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "import numpy as np\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print 'Feature Ranking: '\n",
    "for i in range(16):\n",
    "    print \"{} {} ({})\".format(i+1,features_list[i+1],importances[indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy results in this project is not very meaningful because the number of POI is so few. Recall and Precision for the Decision Tree were 0.28 and 0.30 respectively. I will need to manually choose a subset of the features in order to improve the recall and precision numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick and Tune an Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=75,\n",
      "            splitter='best')\n",
      "\tAccuracy: 0.82560\tPrecision: 0.34140\tRecall: 0.33150\tF1: 0.33638\tF2: 0.33343\n",
      "\tTotal predictions: 15000\tTrue positives:  663\tFalse positives: 1279\tFalse negatives: 1337\tTrue negatives: 11721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Classifier with standard parametres \n",
    "clf = DecisionTreeClassifier(random_state = 75)\n",
    "my_dataset = df[features_list].to_dict(orient = 'index')\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=75, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.88013\tPrecision: 0.61583\tRecall: 0.26850\tF1: 0.37396\tF2: 0.30264\n",
      "\tTotal predictions: 15000\tTrue positives:  537\tFalse positives:  335\tFalse negatives: 1463\tTrue negatives: 12665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest with standard parameters\n",
    "clf = RandomForestClassifier(random_state = 75)\n",
    "clf.fit(df.ix[:,1:], np.ravel(df.ix[:,:1]))\n",
    "\n",
    "# selecting the features with non null importance, sorting and creating features_list for the model\n",
    "features_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0:\n",
    "        features_importance.append([df.columns[i+1], clf.feature_importances_[i]])\n",
    "features_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "features_list = [x[0] for x in features_importance]\n",
    "features_list.insert(0, 'poi')\n",
    "\n",
    "# number of features for best result was found iteratively\n",
    "features_list2 = features_list[:11]\n",
    "my_dataset = df[features_list2].to_dict(orient = 'index')\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list2)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.87213\tPrecision: 0.52774\tRecall: 0.39000\tF1: 0.44853\tF2: 0.41148\n",
      "\tTotal predictions: 15000\tTrue positives:  780\tFalse positives:  698\tFalse negatives: 1220\tTrue negatives: 12302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GaussianNB with feature standartization, selection, PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "# data set standartization\n",
    "scaler = StandardScaler()\n",
    "df_norm = df[features_list]\n",
    "df_norm = scaler.fit_transform(df_norm.ix[:,1:])\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "features_list2 = ['poi']+range(3)\n",
    "my_dataset = pd.DataFrame(SelectKBest(f_classif, k=8).fit_transform(df_norm, df.poi), index = df.index)\n",
    "\n",
    "#PCA\n",
    "pca = PCA(n_components=3)\n",
    "my_dataset2 = pd.DataFrame(pca.fit_transform(my_dataset),  index=df.index)\n",
    "my_dataset2.insert(0, \"poi\", df.poi)\n",
    "my_dataset2 = my_dataset2.to_dict(orient = 'index')  \n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset2, features_list2)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier, Random Forest, and GaussianNB\n",
    "For decision tree and random forest features were selected with non-null importance based on clf.features_importances. Next the number of features were iteratively changed to achieve the best performance. In the GaussianNB classifier feature standization is applied to achieve results, the StandardScaler function is use in the GaussianNB algorithm. In addition, Applied SelectKBest function from sklearn to find k (where k = 8) best features for the algorithm. Finally, Applied PCA to decrease the dimensionality of the data where n_components = 3. Decision tree significantly faster than RandomForest and gave the worst results as shown in table below. These results are before any tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <td>0.8874</td>\n",
       "      <td>0.5761</td>\n",
       "      <td>0.5885</td>\n",
       "      <td>0.5822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.8978</td>\n",
       "      <td>0.7032</td>\n",
       "      <td>0.4040</td>\n",
       "      <td>0.5132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>0.8613</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>0.4496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Accuracy  Precision  Recall      F1\n",
       "Decision Tree Classifier    0.8874     0.5761  0.5885  0.5822\n",
       "Random Forest               0.8978     0.7032  0.4040  0.5132\n",
       "Gaussian Naive Bayes        0.8613     0.4773  0.4250  0.4496"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[0.8874, 0.5761, 0.5885, 0.5822],\n",
    "              [0.8978, 0.7032, 0.4040, 0.5132],\n",
    "              [0.8613, 0.4773, 0.4250, 0.4496]],\n",
    "             columns = ['Accuracy','Precision', 'Recall', 'F1'], \n",
    "             index = ['Decision Tree Classifier', 'Random Forest', 'Gaussian Naive Bayes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tune The Algorithm\n",
    "In machine learning, tuning is working with variable data based on some parameters which have been identified to affect system performance as evaluated by some appropriate metric. Enhanced output reveals which parameter settings are more favorable (tuned) or less favorable (untuned). For the Enron project we tasked with tuning the selected classifier to achieve better than 0.3 precision and 0.3 recall by tuning the parameters in our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion = 'entropy', \n",
    "                             min_samples_split = 19,\n",
    "                             random_state = 60,\n",
    "                             min_samples_leaf=6, \n",
    "                             max_depth = 3,\n",
    "                            class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=6, min_samples_split=19,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=60,\n",
      "            splitter='best')\n",
      "\tAccuracy: 0.92433\tPrecision: 0.77671\tRecall: 0.60700\tF1: 0.68145\tF2: 0.63474\n",
      "\tTotal predictions: 15000\tTrue positives: 1214\tFalse positives:  349\tFalse negatives:  786\tTrue negatives: 12651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(df.ix[:,1:], df.poi)\n",
    "\n",
    "# show the features with non null importance, sorted and create features_list of features for the model\n",
    "features_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0:\n",
    "        features_importance.append([df.columns[i+1], clf.feature_importances_[i]])\n",
    "features_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "features_list = [x[0] for x in features_importance]\n",
    "features_list.insert(0, 'poi')\n",
    "\n",
    "my_dataset = df[features_list].to_dict(orient = 'index')\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Evaluation Metrics\n",
    "\n",
    "Precision is the fraction of retrieved documents that are relevant to the query. For example, for a text search on a set of documents, precision is the number of correct results divided by the number of all returned results. In the Enron project precision can be interpreted as the likelihood that a person who is identified as a POI is actually a true POI; the fact that this is 0.78 means that using this identifier to flag POI’s would result in 22% of the positive flags being false alarms. \n",
    "\n",
    "Recall is the fraction of the relevant documents that are successfully retrieved. For example, for a text search on a set of documents, recall is the number of correct results divided by the number of results that should have been returned. In binary classification, recall is called sensitivity. Recall measures how likely it is that identifier will flag a POI in the test set. 61% of the time it would catch that person, and 39% of the time it wouldn’t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alogrithm Validation Strategy\n",
    "\n",
    "In this project validation is used to evaluating the model  by holding 10% of the data that was not touched during the training process. This is to avoid the mistake of obtaining overly optimistic results due to overfitting the training data, but very poor performance on unseen data. In this project the data is separated into three parts: training, cross-validation and test sets. Cross validation is used to address drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets. The model is tuned to maximize the evaluation score on the cross-validation set, and then the final model efficiency is measured on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.333333333333\n",
      "NB algorithm time: 0.003 s\n"
     ]
    }
   ],
   "source": [
    "features_list = [\"poi\", \"fraction_from_poi_email\", \"fraction_to_poi_email\", \"to_messages\"]\n",
    "\n",
    "### try Naive Bayes for prediction\n",
    "t0 = time()\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "accuracy = accuracy_score(pred,labels_test)\n",
    "print accuracy\n",
    "\n",
    "print \"NB algorithm time:\", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy before tuning  0.76\n",
      "Decision tree algorithm time: 0.002 s\n",
      "done in 0.002s\n",
      "Validating algorithm:\n",
      "accuracy after tuning =  0.88\n",
      "precision =  0.5\n",
      "recall =  0.666666666667\n"
     ]
    }
   ],
   "source": [
    "### features_list is a list of strings, each of which is a feature name\n",
    "### first feature must be \"poi\", as this will be singled out as the label\n",
    "features_list = [\"poi\", \"fraction_from_poi_email\", \"fraction_to_poi_email\"]\n",
    "\n",
    "\n",
    "### store to my_dataset for easy export below\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n",
    "### these two lines extract the features specified in features_list\n",
    "### and extract them from data_dict, returning a numpy array\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "\n",
    "\n",
    "### split into labels and features (this line assumes that the first\n",
    "### feature in the array is the label, which is why \"poi\" must always\n",
    "### be first in features_list\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "### machine learning goes here!\n",
    "### please name your classifier clf for easy export below\n",
    "\n",
    "### deploying feature selection\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "### use KFold for split and validate algorithm\n",
    "from sklearn.cross_validation import KFold\n",
    "kf=KFold(len(labels),3)\n",
    "for train_indices, test_indices in kf:\n",
    "    #make training and testing sets\n",
    "    features_train= [features[ii] for ii in train_indices]\n",
    "    features_test= [features[ii] for ii in test_indices]\n",
    "    labels_train=[labels[ii] for ii in train_indices]\n",
    "    labels_test=[labels[ii] for ii in test_indices]\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(features_train,labels_train)\n",
    "score = clf.score(features_test,labels_test)\n",
    "print 'accuracy before tuning ', score\n",
    "\n",
    "print \"Decision tree algorithm time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "\n",
    "### use manual tuning parameters\n",
    "t0 = time()\n",
    "clf = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "        max_features=None, max_leaf_nodes=None,\n",
    "        min_impurity_decrease=1e-06, min_impurity_split=None,\n",
    "        min_samples_leaf=6, min_samples_split=2,\n",
    "        min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "        splitter='best')\n",
    "clf = clf.fit(features_train,labels_train)\n",
    "pred= clf.predict(features_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "acc=accuracy_score(labels_test, pred)\n",
    "\n",
    "print \"Validating algorithm:\"\n",
    "print \"accuracy after tuning = \", acc\n",
    "\n",
    "# function for calculation ratio of true positives\n",
    "# out of all positives (true + false)\n",
    "print 'precision = ', precision_score(labels_test,pred)\n",
    "\n",
    "# function for calculation ratio of true positives\n",
    "# out of true positives and false negatives\n",
    "print 'recall = ', recall_score(labels_test,pred)\n",
    "\n",
    "\n",
    "### dump your classifier, dataset and features_list so\n",
    "### anyone can run/check your results\n",
    "pickle.dump(clf, open(\"my_classifier.pkl\", \"w\") )\n",
    "pickle.dump(data_dict, open(\"my_dataset.pkl\", \"w\") )\n",
    "pickle.dump(features_list, open(\"my_feature_list.pkl\", \"w\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "    NYTIMES: https://archive.nytimes.com/www.nytimes.com/packages/html/national/20061023_ENRON_TABLE/index.html\n",
    "    Enron data set: https://www.cs.cmu.edu/~./enron/\n",
    "    FindLaw financial data: http://www.findlaw.com\n",
    "    Enron on Wikipedia: https://en.wikipedia.org/wiki/Enron\n",
    "    F1 score on Wikipedia: https://en.wikipedia.org/wiki/F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=1e-06, min_impurity_split=None,\n",
      "            min_samples_leaf=6, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "\tAccuracy: 0.87725\tPrecision: 0.51278\tRecall: 0.36100\tF1: 0.42371\tF2: 0.38372\n",
      "\tTotal predictions: 8000\tTrue positives:  361\tFalse positives:  343\tFalse negatives:  639\tTrue negatives: 6657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load tester.py\n",
    "#!/usr/bin/pickle\n",
    "\n",
    "\"\"\" a basic script for importing student's POI identifier,\n",
    "    and checking the results that they get from it \n",
    " \n",
    "    requires that the algorithm, dataset, and features list\n",
    "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
    "    my_feature_list.pkl, respectively\n",
    "\n",
    "    that process should happen at the end of poi_id.py\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(CLF_PICKLE_FILENAME, \"r\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"r\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"r\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
